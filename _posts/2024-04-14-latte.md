---
title: "Latte: Latent Attention for Linear Time Transformers"
date: 2024-04-14
layout: page
---

<figure style="text-align: center;">
  <img
     src="{{'/assets/img/bid_latte.png' | relative_url }}"
     alt="bid_late"
     class="img-responsive"
     style="float: center;margin-left: 35%;"
    >
  <!--figcaption>MDN Logo</figcaption-->
</figure>

We introduce Latte, a new linear time and memory attention mechanism which, unlike other sparse methods, takes all the tokens into account. Our method can be used as a replacement of the standard attention in both bidirectional and causal settings with similar empirical performance. An additional benefit of causal Latte is efficient inference, which only requires constant time to predict the next token, compared to the linear scaling requirement of the standard attention.

 We are first going to rewrite the classic attention mechanism in the non-vectorises form, which will help us describe the idea behind Latte more intuitively.

 <h2>Quick Recap</h2>

 One of the most common ways of writing an attention layer is using the matrix form:
 \[
 Softmax(\frac{1}{\sqrt{d}}QK^T)V
 \]

 Nonetheles, the formula can be written more intuitivel without any vectorisation. Considering a sequence of $T$ tokens from which we obtain the query $q_i$, key $k_i$ and value $v_i$ we can explain attention using the figure below:


<figure style="text-align: center;">
  <img
     src="{{'/assets/img/att.png' | relative_url }}"
     alt="Attentention Non-Vect"
     class="img-responsive"
     style="float: center;"
    >
    <br>
  <figcaption>Figure1: Non vectorised attention mechanism</figcaption>
</figure>

In the above we defined:
\[A_{ij} = \frac{\exp(Q_{i,:}K_{:,j}^{T})}{\sum_{j=1}^T \exp(Q_{i,:}K_{:,j}^{T}) }\]
and
\[
\tilde{x_i} = \sum_{j=1}^T A_{ij}V_j
\]

For the causal case we would simply sum up to $i$ instead of $T$ such that tokens in the future are not considered. 

<h2> Latte Layer </h2>
The problem with attention is that computing all $A_{ij}$ is quadratic in sequence length, hence we need to find a more efficient way to summarise the interactions between tokens. One simple approach is to intoduce a few learnable tokens and perform attention only between the introduces tokens and the original sequence. This approach is was succesfully explored by many sparse methods like [BigBird](https://arxiv.org/abs/2007.14062), however in the new representation $\tilde{x_i}$ only the learned tokens will contribute.
<figure style="text-align: center;">
  <img
     src="{{'/assets/img/comp_att.png' | relative_url }}"
     alt="Comparison Attentions"
     class="img-responsive"
     style="float: center;"
    >
  <figcaption>Figure2: Comparison between Bidirectional Standard attention, sparse attention methods and Bidirectioanl Latte.</figcaption>
</figure>

More exactly, we replace each $A_{ij}$ with an approximation based on $L$ learnale latent variables:
\[
A_{ij} = \sum_{l=1}^L Q_{il}K_{lj}
\]  
The new vector representation of each token is given by:
\[
\tilde{x_i} = \sum_{l=1}^L Q_{il} \sum_{j=1}^T K_{lj}V_j
\]
Note that $Q$, $K$ have different sizes than the queries and keys in the standard attention. Figure 3 descirbes in detail how we obtain these matrices.

<figure style="text-align: center;">
  <img
     src="{{'/assets/img/fig2.png' | relative_url }}"
     alt="Latte Implementation"
     class="img-responsive"
     style="float: center;"
    >
    <br>
  <figcaption>Figure 3: Each row in X corresponds to a token in the input sentence. We multiply the input to the corresponding weights of Q,K,V. Note that we exponentiate and normalize both Q and K on rows.</figcaption>
</figure>

The approach is motivated by a probabilistic perspective where $A_{i,:}$ is interpreted as probability distribution. While in this blog we describe the model at an intuitive level, a more rigurous mathematical explanation can be found in <a href="https://arxiv.org/abs/2402.17512" target="_blank">our paper</a>

## Causal Latte
In the previous sections we described the bidirectional case, but for problems like language generation we need a causal mechanism. The change can be trivially seen by looking at the formula $\tilde{x_i}$. We only need to consider tokens up until index $i$, and no longer the entire sequence:
\[
\tilde{x_i} = \sum_{l=1}^L Q_{il} \sum_{j=1}^i K_{lj}V_j
\]

This formulation can be implemented using matrix operations, however a sequential implementation has the benefit of small memory and constant time for next token prediction task at inference.

## Results
### Long Range Arena
Long Range Arena os a synthetic benchmark which tests models on long sequences. We implement the tasks with a bidirectional Latte model using 40 latents and obtain good results compared to other transformer models.

| Model | ListOps | Text | Retreival | Image | PathfInder|
|----------------| ---|---|---|--- | --- |
| Latte | 40.18 | 64.5 | 73.39 | 47.55 | 75.61 |
Standard Transformer | 36.37 | 64.27 | 57.46 | 42.44 | 71.40 |
Linformer | 35.70 | 53.94 | 52.27 | 38.56 | 76.34 |
| Luna | 38.01 | 65.74 | 79.55 | 47.47 | 78.89 |
: Table1: Long range arena test set results for bidirectional layer. 

## Language Generation
We train on lnaguage modelling datasets like WikiText-103 and OpenWebText for sequences of size 2048 and 1024 respectively. Preliminary results on the evaluation set are reported in Table 2.

| Model | Wiki103 (PPL) | OpenWebText (PPL)| Enwik-8 (BPC) |
|-------|---|--- |
| Latte | 9.31 (150M) | 19.99 (150M) | 1.28 (36M) |
| GPT2  | 17.48 (1542M) | 18.26 (124M) | 1.11 (44M) |
| Transformer-XL (257M)  |18.3 (257M) | - | 1.06 (41M) |
: Table2: Dev Scores for causal language generation with 128 latent variables. PPL - perplexity score, lower the scores the better

We perform really well on wiki103 text dataset, however, on more complex datasets like OpenWebText or characther level we do not beat state of the art. This can be expected since we have a tradeoff between efficiency of the model and computational complexity. To grasp an understanding of how the model behaves, we show an example of language generation from a short promt.

Prompt:
```
<s>Homarus gammarus
```
Generation of 100 tokens:
```
<s>Homarus gammarus, the common ancestor of the gray wolf <unk>, has been identified to date to at least the Miocene of northern Chile, from the Lower Peru. Analysis by the researchers has shown the presence of <unk>, <unk>, <unk>, and <unk> <unk> to be a member of the subfamily <unk>, although <unk> et al. ( 2001 ) suggested a species may represent this subspecies. \n fauna includes large mammals, and a few species have been documented in the Pacific island of California. \n mtDNA from Arizona bears the subspecies D. huenei's skull, found across the Pacific Ocean, along with three species of small theropod mongooses. The two species share several distinct forms, both geographically and genetically. The genus name honors the long @-@ established genus <unk>, found in northwestern Argentina. The skull and skull are from the Late Cretaceous of Texas, but not from"
```

While the facts are not correct, the language is fluent. Also we have <unk> tokens because the data was pre-processed to eliminate entities, hence <unk> token was present in the training set. 
