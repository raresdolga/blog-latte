---
title: "Latte: Latent Attention for Linear Time Transformers"
date: 2024-04-14
layout: page
---

<figure style="text-align: center;">
  <img
     src="{{'/assets/img/bid_latte.png' | relative_url }}"
     alt="bid_late"
     class="img-responsive"
     style="float: center;margin-left: 35%;"
    >
  <!--figcaption>MDN Logo</figcaption-->
</figure>

Transformers are one of the most popular architectures used in both sequence modelling and computer vision. At the center of transformers is the attention mechanism which compares each element of a sequence with every other element. This pairwise similarity score is used to decide how much the other tokens contribute to the new representation of one element. While the approach gives state-of-the-art results, it comes at the cost of quadratic time complexity. Additionally, for language generation, the next token prediction is linear in the prompt length, compared to the constant time complexity of approaches like RNNs and SSMs. We introduce Latte, a new linear time and memory replacement for attention, which achieves a comparable performance to transformers while being more efficient during training and inference. In this blog post, we focus on an intuitive explanation of Latte, but the approach is inspired and can be easily understood from the lens of latent variables. For a concise mathematical explanation, check out <a href="https://arxiv.org/abs/2402.17512" target="_blank">our paper</a>.

We are first going to rewrite the classic attention mechanism in the non-vectorised form, which will help us describe the idea behind Latte.

<h2>Quick Recap</h2>

One of the most common ways of writing an attention layer is using the matrix form:
 \[
 \text{Softmax} \bigg ( \frac{1}{\sqrt{d}}QK^\mathsf{T} \bigg )V
 \]

Nonetheless, bearing in mind that attention is based on pairwise interactions between elements of a sequence, the formula can be written more intuitively without any vectorisation. Considering a sequence of $T$ tokens from which we obtain the queries $q_t$, keys $k_t$ and values $v_t$ we can explain attention using the figure below:

<figure style="text-align: center;">
  <img
     src="{{'/assets/img/att.png' | relative_url }}"
     alt="Attentention Non-Vect"
     class="img-responsive"
     style="float: center;"
    >
    <br>
  <figcaption>Figure1: Non-vectorised attention mechanism</figcaption>
</figure>

Mathematically, the above can be written to:
\[a_{ts} = \frac{\exp(q_t^\mathsf{T}k_s)}{\sum_{s=1}^T \exp(q_t^\mathsf{T}k_s) } := p(s|t) \]
and
\[\begin{aligned} {\tilde{x}}\_{t} &= \sum_{s=1}^T a_{ts}v_s \\\ &= \sum_{s=1}^T p(s|t) v_s \end{aligned}\]

Hence, the new representation of ${\tilde{x}}\_{t}$ is a combination of all elements, weighted by their similarity with the current token $x_t$. To make the connection with the matrix form, we observe that $q_t^\mathsf{T} = Q_{t,:}$ and $k_s = K^\mathsf{T}_{:,s}$. Notice that we can think of $a _{ts} $ as the probability of occurrence for a token at position $ s $ given the token at position $ t $. This observation will help us understand our latent approach. Here we defined bidirectional attention, but for the causal case, we would simply sum up to index $t$ instead of the entire sequence length $T$, such that tokens in the future are not considered. 

<h2> Latte Layer </h2>
As previously stated the bottleneck of the attention is computing the weights $a_{ts}$. We mitigate this by introducing learnable latent tokens which are compared to each element of the sequence. Since the number of latent tokens is fixed, the cost of computation becomes linear in the sequence length. Intuitively, we can think of the latents as concepts like colours or shapes, to which we can compare the elements of the input. Then we can create a new representation using all the sequence tokens and their similarity to the learned high-level concepts. In Figure 2, we show the difference between bidirectional Latte, bidirectional Standard Attention and Sparse Attention methods which we will briefly describe next.

<figure style="text-align: center;">
  <img
     src="{{'/assets/img/comp_att.png' | relative_url }}"
     alt="Comparison Attentions"
     class="img-responsive"
     style="float: center;"
    >
  <figcaption>Figure 2: Comparison between Bidirectional Standard Attention, Sparse Attention and Bidirectional Latte.</figcaption>
</figure>

The approach has similarities with sparse attention methods, which only compute attention between a set of learnable global tokens and all the sequence elements. However, the main difference is that the sparse methods are weighted sums of the global tokens, while in our approach we consider the entire sequence. Specifically, we approximate the full attention using the latents, instead of only performing attention between the latents and the sequence elements. 

Mathematically, we replace each $a_{ts}$ with an approximation based on $L$ learnable latent variables:
\[ \begin{aligned} a_{ts} = p(s|t) &= \sum_{l=1}^L p(s,l|t) \\\ &= \sum_{l=1}^L p(l|t) p(s|l) \end{aligned} \]  
In the above, we assumed independence between $ s $ and $ t $ give $ l $. Intuitively we compute the similarity between a high-level concept and each element, then we re-weight it based on the similarity between our current token and the high-level concepts. We can reuse the attention matrices $ Q $ and $ K $ for calculating the probabilities above, giving us the new vector representation of each token:
\[
\tilde{X} = \text{Softmax} ( Q ) \text{Softmax}(K)^\mathsf{T} V
\]

Note that $Q$ and $K$ have different sizes than the queries and keys in the standard attention. Figure 3 describes in detail how we obtain these matrices.

<figure style="text-align: center;">
  <img
     src="{{'/assets/img/fig2.png' | relative_url }}"
     alt="Latte Implementation"
     class="img-responsive"
     style="float: center;"
    >
    <br>
  <figcaption> Figure 3: Each row in $X$ corresponds to a token in the input sentence. We multiply the input by the corresponding weights and obtain $Q$, $K$, $V$. We exponentiate and normalize $Q$ on columns across all latents and $K$ on rows across the sequence. </figcaption>
</figure>

The approach is not entirely new as other works have decomposed attention in the same fashion for the bidirectional case. However, the probabilistic approach easily allows us to extend our model to the causal case.

## Causal Latte
In the previous sections, we described the bidirectional case, but for problems like language generation, we need a causal mechanism. The change can be trivially seen by looking at the formula for $\tilde{x}\_t$ and only sum up to index $t$ instead of the entire sequence. This means that we have a cumulative sum and we cannot simply apply the softmax function over $K$. Instead, we need an approach in which the normalisation factor and the weight given by $K$ are updated sequentially. Considering $Q^{'} =  \text{Softmax} ( Q ) $ and $K^{'} = \exp(K)$ we can write
\[
\tilde{x}\_t = \sum_{l=1}^L Q_{tl}^{'} \frac{1}{\sum_{s=1}^t K_{ls}^{'}} \sum_{s=1}^t K_{ls}^{'}V_{s,:}
\]

This formulation can be implemented using matrix operations, however, a sequential implementation has the benefit of constant time and memory complexity for the next token prediction task at inference.

## Results
### Runtime Efficiency
We developed a method with linear time and memory complexity in the sequence length. The only possible drawback is that the causal version needs to be implemented sequentially to decrease memory usage and have constant time inference. If the sequence length is small, this can be slower than a vectorised version of standard attention on GPUs. To see the benefits of Latte, we perform an analysis of runtime performance in Figure 4.
<figure style="text-align: center;">
  <img
     src="{{'/assets/img/eff.png' | relative_url }}"
     alt="Efficiency"
     class="img-responsive"
     style="float: center;"
    >
    <br>
  <figcaption>Figure 4: Time and memory comparison of bidirectional and casual standard attention and Latte. For the former, we also test its sequential implementation with linear memory complexity. We set the number of latents to $L = 128$ while the rest of the hyperparameters are the same as the ones used in the standard attention.</figcaption>
</figure>

From the above, we can see that the bidirectional case is more efficient even when the sequence length is small. However, the sequential causal model becomes more efficient for sequences longer than 3000 tokens. In terms of memory, Latte is more efficient even when the sequence has a smaller length.

### Long Range Arena
Long Range Arena is a synthetic benchmark which tests models on long sequences. We implement the tasks with a bidirectional Latte model using 40 latents and show that we outperform the standard attention. We also compare to other efficient transformers and obtain comparable results, with the benefit that our method could easily be applied in both causal and bidirectional cases.

| Model | ListOps | Text | Retreival | Image | PathfInder|
|----------------| ---|---|---|--- | --- |
| Latte | 40.18 | 64.5 | 73.39 | 47.55 | 75.61 |
Standard Transformer | 36.37 | 64.27 | 57.46 | 42.44 | 71.40 |
Linformer | 35.70 | 53.94 | 52.27 | 38.56 | 76.34 |
| Luna | 38.01 | 65.74 | 79.55 | 47.47 | 78.89 |

Table1: Long Range Arena test set results for bidirectional layer. 

## Language Generation
We train on language modelling datasets like Enwik8, WikiText-103 and OpenWebText for sequences of size 2048 and 1024 respectively. Preliminary results on the evaluation set are reported in Table 2.

| Model | Wiki103 (PPL) | OpenWebText (PPL)| Enwik-8 (BPC) |
|-------|---|--- |---|
| Latte | 9.31 (150M) | 19.04 (150M) | 1.28 (36M) |
| Retro | 3.21 (7,532M) | - | - |
| GPT2  | 17.48 (1542M) | 18.26 (124M) | 1.11 (44M) |
| Transformer-XL  |18.3 (257M) | - | 1.06 (41M) |

Table 2: Dev Scores for causal language generation with 128 latent variables. PPL - perplexity score, the lower the scores the better.

On the Wiki103 text dataset, Latte outperforms other models by far. We also compare with Retro, to show that lower perplexities on Wiki103 are possible. However, it is important to note that such models use external information to generate text, such as billions of cleaned text tokens. Hence they do not only depend on the user prompt like our model. On more complex datasets like OpenWebText or character-level like Enwik8, our model does not beat the state of the art. This can be expected since we have a tradeoff between the performance of the model and its computational complexity.

# Final Thoughts
Inspired by the fact that language can be decomposed into higher-level concepts, we developed a linear time and memory approach to replace attention in both bidirectional and causal cases. Another benefit of our approach is that the next token prediction task runs in constant time, resulting in a fast model during generation. We showed that our model has better performance than the standard transformer on bidirectional tasks like Long Range Arena, and comparable results for language modelling.
<!-- Prompt:
```
<s>Homarus gammarus
```
Generation of 100 tokens:
```
<s>Homarus gammarus, the common ancestor of the gray wolf <unk>, has been identified to date to at least the Miocene of northern Chile, from the Lower Peru. Analysis by the researchers has shown the presence of <unk>, <unk>, <unk>, and <unk> <unk> to be a member of the subfamily <unk>, although <unk> et al. ( 2001 ) suggested a species may represent this subspecies. \n fauna includes large mammals, and a few species have been documented in the Pacific island of California. \n mtDNA from Arizona bears the subspecies D. huenei's skull, found across the Pacific Ocean, along with three species of small theropod mongooses. The two species share several distinct forms, both geographically and genetically. The genus name honors the long @-@ established genus <unk>, found in northwestern Argentina. The skull and skull are from the Late Cretaceous of Texas, but not from"
```

While the facts are not correct, the language is fluent. Also, we have < unk > tokens because the data was pre-processed to eliminate entities hence the < unk > token was present in the training set.  -->
